{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9f530a-d336-4dc1-bbcd-734d127b3213",
   "metadata": {},
   "source": [
    "# Description\n",
    "In this file we will examine the percentage of plural and non plural. Also the precentage of usage of tenses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdad63d-3510-4552-9925-7c248a444725",
   "metadata": {},
   "source": [
    "# Read data\n",
    "In order to save time, we will only load some part of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112a3ca1-3084-4251-ac12-a1afa5339fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "name_table = \"NameTable\"\n",
    "conn = sqlite3.connect('../ZipfLawAnalysis/data.db')\n",
    "query = f\"\"\"SELECT *\n",
    "FROM (\n",
    "    SELECT *\n",
    "    FROM {name_table}\n",
    "    WHERE authorLocation = 'China'\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT 100000\n",
    ") AS Chinese_sample\n",
    "UNION ALL\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *\n",
    "    FROM {name_table}\n",
    "    WHERE authorLocation = 'USA'\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT 100000\n",
    ") AS USA_sample;\"\"\"\n",
    "# takes 10 second to run 100k\n",
    "df = pd.read_sql_query(query, conn)\n",
    "import json\n",
    "df['terms'] = df.terms.apply(json.loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf7e60c-fdb5-4928-b1db-cbaeea78b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>nameType</th>\n",
       "      <th>nameScope</th>\n",
       "      <th>projectSize</th>\n",
       "      <th>authorName</th>\n",
       "      <th>authorProficiency</th>\n",
       "      <th>authorLocation</th>\n",
       "      <th>terms</th>\n",
       "      <th>namingConvention</th>\n",
       "      <th>actual_terms</th>\n",
       "      <th>standarized_name</th>\n",
       "      <th>atual_standarized_name</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1455146</td>\n",
       "      <td>test_shadowing_for_tuple_1</td>\n",
       "      <td>function</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>71103</td>\n",
       "      <td>wr786</td>\n",
       "      <td>50..100</td>\n",
       "      <td>China</td>\n",
       "      <td>[test, shadowing, for, tuple]</td>\n",
       "      <td>Snake</td>\n",
       "      <td>[test, shadowing, for, tuple]</td>\n",
       "      <td>test_shadowing_for_tuple</td>\n",
       "      <td>test_shadowing_for_tuple</td>\n",
       "      <td>test shadowing for tuple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3334420</td>\n",
       "      <td>test_constant_strategy_regressor</td>\n",
       "      <td>function</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>105670</td>\n",
       "      <td>zhongmicai</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>China</td>\n",
       "      <td>[test, constant, strategy, regressor]</td>\n",
       "      <td>Snake</td>\n",
       "      <td>[test, constant, strategy, regressor]</td>\n",
       "      <td>test_constant_strategy_regressor</td>\n",
       "      <td>test_constant_strategy_regressor</td>\n",
       "      <td>test constant strategy regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3953784</td>\n",
       "      <td>get_statement_hint_text</td>\n",
       "      <td>function</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>11199</td>\n",
       "      <td>chenlongzhen</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>China</td>\n",
       "      <td>[get, statement, hint, text]</td>\n",
       "      <td>Snake</td>\n",
       "      <td>[get, statement, hint, text]</td>\n",
       "      <td>get_statement_hint_text</td>\n",
       "      <td>get_statement_hint_text</td>\n",
       "      <td>get statement hint text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3113064</td>\n",
       "      <td>_set_interrupt</td>\n",
       "      <td>function</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>59359</td>\n",
       "      <td>holynova</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>China</td>\n",
       "      <td>[set, interrupt]</td>\n",
       "      <td>Snake</td>\n",
       "      <td>[set, interrupt]</td>\n",
       "      <td>set_interrupt</td>\n",
       "      <td>set_interrupt</td>\n",
       "      <td>set interrupt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36587</td>\n",
       "      <td>tempdir</td>\n",
       "      <td>function</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>91720</td>\n",
       "      <td>brightmart</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>China</td>\n",
       "      <td>[tempdir]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[temporary, directory]</td>\n",
       "      <td>tempdir</td>\n",
       "      <td>temporary_directory</td>\n",
       "      <td>temporary directory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>5550380</td>\n",
       "      <td>end</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>14462</td>\n",
       "      <td>vivekaxl</td>\n",
       "      <td>50..100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[end]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[end]</td>\n",
       "      <td>end</td>\n",
       "      <td>end</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>5593790</td>\n",
       "      <td>__radd__</td>\n",
       "      <td>function</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>14462</td>\n",
       "      <td>vivekaxl</td>\n",
       "      <td>50..100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[radd]</td>\n",
       "      <td>Snake</td>\n",
       "      <td>[right, add]</td>\n",
       "      <td>radd</td>\n",
       "      <td>right_add</td>\n",
       "      <td>right add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>6758081</td>\n",
       "      <td>culprit</td>\n",
       "      <td>variable</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>7510</td>\n",
       "      <td>asweigart</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[culprit]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[culprit]</td>\n",
       "      <td>culprit</td>\n",
       "      <td>culprit</td>\n",
       "      <td>culprit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>4237553</td>\n",
       "      <td>prob</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>3239</td>\n",
       "      <td>dqwang122</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>USA</td>\n",
       "      <td>[prob]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[prob]</td>\n",
       "      <td>prob</td>\n",
       "      <td>prob</td>\n",
       "      <td>prob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>6840245</td>\n",
       "      <td>item_goldHandClockID</td>\n",
       "      <td>variable</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>3885</td>\n",
       "      <td>seanpm2001</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[item, gold, ID, Hand, Clock]</td>\n",
       "      <td>non-convention</td>\n",
       "      <td>[item, gold, id, hand, clock]</td>\n",
       "      <td>item_gold_id_hand_clock</td>\n",
       "      <td>item_gold_id_hand_clock</td>\n",
       "      <td>item gold id hand clock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                              name  nameType      nameScope  \\\n",
       "0       1455146        test_shadowing_for_tuple_1  function  FunctionScope   \n",
       "1       3334420  test_constant_strategy_regressor  function    GlobalScope   \n",
       "2       3953784           get_statement_hint_text  function  FunctionScope   \n",
       "3       3113064                    _set_interrupt  function  FunctionScope   \n",
       "4         36587                           tempdir  function  FunctionScope   \n",
       "...         ...                               ...       ...            ...   \n",
       "199995  5550380                               end  variable  FunctionScope   \n",
       "199996  5593790                          __radd__  function  FunctionScope   \n",
       "199997  6758081                           culprit  variable    GlobalScope   \n",
       "199998  4237553                              prob  variable  FunctionScope   \n",
       "199999  6840245              item_goldHandClockID  variable    GlobalScope   \n",
       "\n",
       "        projectSize    authorName authorProficiency authorLocation  \\\n",
       "0             71103         wr786           50..100          China   \n",
       "1            105670    zhongmicai              >100          China   \n",
       "2             11199  chenlongzhen              >100          China   \n",
       "3             59359      holynova              >100          China   \n",
       "4             91720    brightmart               <50          China   \n",
       "...             ...           ...               ...            ...   \n",
       "199995        14462      vivekaxl           50..100            USA   \n",
       "199996        14462      vivekaxl           50..100            USA   \n",
       "199997         7510     asweigart              >100            USA   \n",
       "199998         3239     dqwang122               <50            USA   \n",
       "199999         3885    seanpm2001              >100            USA   \n",
       "\n",
       "                                        terms namingConvention  \\\n",
       "0               [test, shadowing, for, tuple]            Snake   \n",
       "1       [test, constant, strategy, regressor]            Snake   \n",
       "2                [get, statement, hint, text]            Snake   \n",
       "3                            [set, interrupt]            Snake   \n",
       "4                                   [tempdir]          Unknown   \n",
       "...                                       ...              ...   \n",
       "199995                                  [end]          Unknown   \n",
       "199996                                 [radd]            Snake   \n",
       "199997                              [culprit]          Unknown   \n",
       "199998                                 [prob]          Unknown   \n",
       "199999          [item, gold, ID, Hand, Clock]   non-convention   \n",
       "\n",
       "                                 actual_terms  \\\n",
       "0               [test, shadowing, for, tuple]   \n",
       "1       [test, constant, strategy, regressor]   \n",
       "2                [get, statement, hint, text]   \n",
       "3                            [set, interrupt]   \n",
       "4                      [temporary, directory]   \n",
       "...                                       ...   \n",
       "199995                                  [end]   \n",
       "199996                           [right, add]   \n",
       "199997                              [culprit]   \n",
       "199998                                 [prob]   \n",
       "199999          [item, gold, id, hand, clock]   \n",
       "\n",
       "                        standarized_name            atual_standarized_name  \\\n",
       "0               test_shadowing_for_tuple          test_shadowing_for_tuple   \n",
       "1       test_constant_strategy_regressor  test_constant_strategy_regressor   \n",
       "2                get_statement_hint_text           get_statement_hint_text   \n",
       "3                          set_interrupt                     set_interrupt   \n",
       "4                                tempdir               temporary_directory   \n",
       "...                                  ...                               ...   \n",
       "199995                               end                               end   \n",
       "199996                              radd                         right_add   \n",
       "199997                           culprit                           culprit   \n",
       "199998                              prob                              prob   \n",
       "199999           item_gold_id_hand_clock           item_gold_id_hand_clock   \n",
       "\n",
       "                                  phrase  \n",
       "0               test shadowing for tuple  \n",
       "1       test constant strategy regressor  \n",
       "2                get statement hint text  \n",
       "3                          set interrupt  \n",
       "4                    temporary directory  \n",
       "...                                  ...  \n",
       "199995                               end  \n",
       "199996                         right add  \n",
       "199997                           culprit  \n",
       "199998                              prob  \n",
       "199999           item gold id hand clock  \n",
       "\n",
       "[200000 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbrev_table = \"AbbreviationMap\"\n",
    "query = f\"SELECT * FROM {abbrev_table}\"\n",
    "df_abbrev_map = pd.read_sql_query(query, conn)\n",
    "\n",
    "# I will use a better dictionary: ENABLE (Enhanced North American Benchmark Lexicon)\n",
    "with open('../ZipfLawAnalysis/SavedFiles/atebits.txt', 'r') as file:\n",
    "    words = file.read().splitlines()\n",
    "english_dictionary =  set(words)\n",
    "\n",
    "# the dictionary that maps abbreviation back to original words\n",
    "abbrev_map = dict(zip(df_abbrev_map['term'], df_abbrev_map['abbrev_meaning']))\n",
    "# because the confidence of preicting single letter is too low, I would give up all the single letters\n",
    "# also there are ones that ChatGPT cannot recognize, generally too wierd ones, so I will get rid of those too. (277 of them)\n",
    "# also there are about 20k duplicates due to capitalization, here we will combine them together first. ???\n",
    "filtered_abbrev_map = {k: v for k, v in abbrev_map.items() if v != '-1'}\n",
    "\n",
    "# function that checks if it's a real word\n",
    "def lookup_terms(term):\n",
    "    return term.lower() in english_dictionary\n",
    "\n",
    "def map_terms_to_actual_terms(terms):\n",
    "    # if it's dictionary word, it will not be in the dictionary, or it might be something that GPT cannot guess. \n",
    "    # either way, the original terms will be in the list. Else, the translated terms will be in the list.\n",
    "    return [filtered_abbrev_map.get(term, term) for term in terms]\n",
    "\n",
    "df['actual_terms'] = df['terms'].apply(map_terms_to_actual_terms)   \n",
    "\n",
    "temp = df['terms'].apply('_'.join).str.lower()\n",
    "df['standarized_name'] = temp\n",
    "\n",
    "temp = df['actual_terms'].apply('_'.join).str.lower().str.replace(\" \", \"_\")\n",
    "df['atual_standarized_name'] = temp\n",
    "\n",
    "# we use atual_standarized_name to define actual_terms so that we can get rid of the space\n",
    "# sometimes pd will return view of df not the actual df, depends on the RAM\n",
    "df = df.copy()\n",
    "df['actual_terms'] = df['atual_standarized_name'].apply(lambda x: x.split('_'))\n",
    "\n",
    "# phrase is basically the atual_standarized_name but connect by space\n",
    "df[\"phrase\"] = df[\"actual_terms\"].apply(\" \".join)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4e576-27d0-4503-85d7-cb1554e691d7",
   "metadata": {},
   "source": [
    "# Plural and Tense Detection\n",
    "We are using the princeton wordnet instead of the NLTK wordnet? Let's use the NLTK one for now\n",
    "But we are suppose to use the NLTK for pos tagging (We could also use spacy as in MPM.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e2220a-3dba-482d-a25e-e750017d5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1355104d-4e95-4b58-86fd-edc2283feaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "    # Perform POS tagging on the tokenized words\n",
    "    pos_tags = pos_tag(words)\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "df['pos_tag'] = df[\"phrase\"].apply(pos_tagging)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd74a25e-050f-4f2f-b761-ba21ec1e5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def count_plural_and_tense(list_word_pos):\n",
    "    plural_count, tense_count = 0, 0\n",
    "    num_n, num_v = 0, 0\n",
    "    for word, pos in list_word_pos:\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        singular = wordnet.morphy(word, pos=wordnet_pos)\n",
    "        if singular is not None and singular != word:\n",
    "            # if it's a noun, then it's plural, if it's a verb, then it's different tense\n",
    "            if wordnet_pos == wordnet.NOUN:\n",
    "                plural_count += 1\n",
    "                # print(\"noun\", word, singular) # for debug\n",
    "            if wordnet_pos == wordnet.VERB:\n",
    "                tense_count += 1\n",
    "                # print('verb', word, singular) # for debug\n",
    "        if wordnet_pos == wordnet.NOUN:\n",
    "            num_n += 1\n",
    "        if wordnet_pos == wordnet.VERB:\n",
    "            num_v += 1\n",
    "\n",
    "    return plural_count, num_n, tense_count, num_v\n",
    "        \n",
    "temp = df['pos_tag'].apply(count_plural_and_tense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9571932-0e95-450c-87ed-c6f00ad8aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"num_plural\", \"total_n\", \"num_tense\", \"total_v\"]] = pd.DataFrame(temp.to_list(), columns=[\"num_plural\", \"total_n\", \"num_tense\", \"total_v\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50564400-ba3a-4b95-b6dc-387f72cda2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "num_plural: 14671           total_n: 166769          percentage_plural: 0.08797198520108654\n",
      "num_tense: 7487            total_v: 17161           percentage:0.4362799370666045 \n",
      "USA\n",
      "num_plural: 15120           total_n: 171566          percentage_plural: 0.08812934963804017\n",
      "num_tense: 8333            total_v: 17876           percentage:0.46615573953904677 \n"
     ]
    }
   ],
   "source": [
    "# get the chinese subset and USA subset\n",
    "df_china = df[df['authorLocation'] == 'China']\n",
    "df_usa = df[df['authorLocation'] == 'USA']\n",
    "\n",
    "for x, df_x in [('China', df_china), ('USA',df_usa)]:\n",
    "    num_plural = df_x['num_plural'].sum()\n",
    "    num_tense = df_x['num_tense'].sum()\n",
    "    total_n = df_x['total_n'].sum()\n",
    "    total_v = df_x['total_v'].sum()\n",
    "    percentage_plural = num_plural/total_n\n",
    "    percentage_tense = num_tense/total_v\n",
    "    print(x)\n",
    "    print(f\"num_plural: {num_plural: <15} total_n: {total_n:<15} percentage_plural: {percentage_plural: <15}\")\n",
    "    print(f\"num_tense: {num_tense:<15} total_v: {total_v:<15} percentage:{percentage_tense: <15} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64292b-2294-4f56-9571-3ca90934820e",
   "metadata": {},
   "source": [
    "Okay.... Seems pretty bad, lets see the original terms...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ccd8cb-877a-48de-bcd4-14814386f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "num_plural: 10617           total_n: 152583          percentage_plural: 0.0695818013802324\n",
      "num_tense: 4833            total_v: 12366           percentage:0.39082969432314413 \n",
      "USA\n",
      "num_plural: 10144           total_n: 156170          percentage_plural: 0.064954856886726\n",
      "num_tense: 4990            total_v: 11657           percentage:0.42806897143347344 \n"
     ]
    }
   ],
   "source": [
    "df[\"phrase\"] = df[\"terms\"].apply(\" \".join)\n",
    "df['pos_tag'] = df[\"phrase\"].apply(pos_tagging)\n",
    "temp = df['pos_tag'].apply(count_plural_and_tense)\n",
    "df[[\"num_plural\", \"total_n\", \"num_tense\", \"total_v\"]] = pd.DataFrame(temp.to_list(), columns=[\"num_plural\", \"total_n\", \"num_tense\", \"total_v\"]).copy()# get the chinese subset and USA subset\n",
    "df_china = df[df['authorLocation'] == 'China']\n",
    "df_usa = df[df['authorLocation'] == 'USA']\n",
    "\n",
    "for x, df_x in [('China', df_china), ('USA',df_usa)]:\n",
    "    num_plural = df_x['num_plural'].sum()\n",
    "    num_tense = df_x['num_tense'].sum()\n",
    "    total_n = df_x['total_n'].sum()\n",
    "    total_v = df_x['total_v'].sum()\n",
    "    percentage_plural = num_plural/total_n\n",
    "    percentage_tense = num_tense/total_v\n",
    "    print(x)\n",
    "    print(f\"num_plural: {num_plural: <15} total_n: {total_n:<15} percentage_plural: {percentage_plural: <15}\")\n",
    "    print(f\"num_tense: {num_tense:<15} total_v: {total_v:<15} percentage:{percentage_tense: <15} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1460e-6e7f-48f5-aced-3fd99126f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
