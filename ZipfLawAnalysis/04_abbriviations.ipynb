{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d089aa-8839-4d5e-a2af-2d8bb2c9ffd1",
   "metadata": {},
   "source": [
    "# Description\n",
    "This file is used to convert abbriviation on name back into dictionary words. There are two approaches that we are going to try: the first approach is to use a dictionary that maps abbriviations to dictionary words. The second approach is pass it into LLMs and see how would the LLM guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8ceaf-6d8a-4295-aef9-632bc6298833",
   "metadata": {},
   "source": [
    "## Read Files\n",
    "First we will read the result from previous work, in which we get python files from internet, parse them to get all the variable and function names (and their scope, might be useful later), and then we parse the variable and function names into terms (in a primitive way). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745e5254-c911-437f-93ce-00aa76b9d356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:27.416783Z",
     "start_time": "2024-06-08T14:47:37.532652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>nameType</th>\n",
       "      <th>nameScope</th>\n",
       "      <th>projectSize</th>\n",
       "      <th>authorName</th>\n",
       "      <th>authorProficiency</th>\n",
       "      <th>authorLocation</th>\n",
       "      <th>terms</th>\n",
       "      <th>namingConvention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_raise_err</td>\n",
       "      <td>function</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>72400</td>\n",
       "      <td>programthink</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>China</td>\n",
       "      <td>[raise, err]</td>\n",
       "      <td>Snake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>_load_yaml</td>\n",
       "      <td>function</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>72400</td>\n",
       "      <td>programthink</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>China</td>\n",
       "      <td>[load, yaml]</td>\n",
       "      <td>Snake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Node</td>\n",
       "      <td>class</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>72400</td>\n",
       "      <td>programthink</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>China</td>\n",
       "      <td>[Node]</td>\n",
       "      <td>Pascal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Relation</td>\n",
       "      <td>class</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>72400</td>\n",
       "      <td>programthink</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>China</td>\n",
       "      <td>[Relation]</td>\n",
       "      <td>Pascal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Family</td>\n",
       "      <td>class</td>\n",
       "      <td>GlobalScope</td>\n",
       "      <td>72400</td>\n",
       "      <td>programthink</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>China</td>\n",
       "      <td>[Family]</td>\n",
       "      <td>Pascal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234210</th>\n",
       "      <td>8234210</td>\n",
       "      <td>__long__</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>10460</td>\n",
       "      <td>juvers</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[long]</td>\n",
       "      <td>Snake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234211</th>\n",
       "      <td>8234211</td>\n",
       "      <td>__getslice__</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>10460</td>\n",
       "      <td>juvers</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[getslice]</td>\n",
       "      <td>Snake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234212</th>\n",
       "      <td>8234212</td>\n",
       "      <td>__setslice__</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>10460</td>\n",
       "      <td>juvers</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[setslice]</td>\n",
       "      <td>Snake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234213</th>\n",
       "      <td>8234213</td>\n",
       "      <td>__delslice__</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>10460</td>\n",
       "      <td>juvers</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[delslice]</td>\n",
       "      <td>Snake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234214</th>\n",
       "      <td>8234214</td>\n",
       "      <td>factory</td>\n",
       "      <td>variable</td>\n",
       "      <td>FunctionScope</td>\n",
       "      <td>10460</td>\n",
       "      <td>juvers</td>\n",
       "      <td>&gt;100</td>\n",
       "      <td>USA</td>\n",
       "      <td>[factory]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8234215 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id          name  nameType      nameScope  projectSize  \\\n",
       "0              0    _raise_err  function    GlobalScope        72400   \n",
       "1              1    _load_yaml  function    GlobalScope        72400   \n",
       "2              2          Node     class    GlobalScope        72400   \n",
       "3              3      Relation     class    GlobalScope        72400   \n",
       "4              4        Family     class    GlobalScope        72400   \n",
       "...          ...           ...       ...            ...          ...   \n",
       "8234210  8234210      __long__  variable  FunctionScope        10460   \n",
       "8234211  8234211  __getslice__  variable  FunctionScope        10460   \n",
       "8234212  8234212  __setslice__  variable  FunctionScope        10460   \n",
       "8234213  8234213  __delslice__  variable  FunctionScope        10460   \n",
       "8234214  8234214       factory  variable  FunctionScope        10460   \n",
       "\n",
       "           authorName authorProficiency authorLocation         terms  \\\n",
       "0        programthink               <50          China  [raise, err]   \n",
       "1        programthink               <50          China  [load, yaml]   \n",
       "2        programthink               <50          China        [Node]   \n",
       "3        programthink               <50          China    [Relation]   \n",
       "4        programthink               <50          China      [Family]   \n",
       "...               ...               ...            ...           ...   \n",
       "8234210        juvers              >100            USA        [long]   \n",
       "8234211        juvers              >100            USA    [getslice]   \n",
       "8234212        juvers              >100            USA    [setslice]   \n",
       "8234213        juvers              >100            USA    [delslice]   \n",
       "8234214        juvers              >100            USA     [factory]   \n",
       "\n",
       "        namingConvention  \n",
       "0                  Snake  \n",
       "1                  Snake  \n",
       "2                 Pascal  \n",
       "3                 Pascal  \n",
       "4                 Pascal  \n",
       "...                  ...  \n",
       "8234210            Snake  \n",
       "8234211            Snake  \n",
       "8234212            Snake  \n",
       "8234213            Snake  \n",
       "8234214          Unknown  \n",
       "\n",
       "[8234215 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "name_table = \"NameTable\"\n",
    "conn = sqlite3.connect('data.db')\n",
    "query = f\"SELECT * FROM {name_table}\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "import json\n",
    "df['terms'] = df.terms.apply(json.loads)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d6715-ea00-4ab0-8d7a-3c402d261585",
   "metadata": {},
   "source": [
    "## Get terms and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fec2f06-2c8a-4599-ad01-71a8a2c24240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:48.681489Z",
     "start_time": "2024-06-08T14:49:28.891847Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all the terms in Chinese dataframe\n",
    "flattened_words_chinese = df[df['authorLocation'] == 'China']['terms'].explode()\n",
    "# Count the occurrences of each unique terms\n",
    "word_counts_chinese = flattened_words_chinese.value_counts()\n",
    "df_chinese_word_freq = pd.DataFrame(word_counts_chinese.reset_index())\n",
    "\n",
    "# do the same for Americans\n",
    "flattened_words_english = df[df['authorLocation'] == 'USA']['terms'].explode()\n",
    "# Count the occurrences of each word\n",
    "word_counts_english = flattened_words_english.value_counts()\n",
    "df_english_word_freq = pd.DataFrame(word_counts_english.reset_index())\n",
    "\n",
    "# do the total words\n",
    "flattened_words = df['terms'].explode()\n",
    "# Count the occurrences of each word\n",
    "word_counts = flattened_words.value_counts()\n",
    "df_word_freq = pd.DataFrame(word_counts.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60055282-486a-44ee-8872-716834bc5668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:48.693891Z",
     "start_time": "2024-06-08T14:49:48.676242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>366742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>init</td>\n",
       "      <td>173722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get</td>\n",
       "      <td>171985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>name</td>\n",
       "      <td>113341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>97638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138447</th>\n",
       "      <td>bistochastize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138448</th>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138449</th>\n",
       "      <td>Toward</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138450</th>\n",
       "      <td>MICROS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138451</th>\n",
       "      <td>itrin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                terms   count\n",
       "0                test  366742\n",
       "1                init  173722\n",
       "2                 get  171985\n",
       "3                name  113341\n",
       "4                  ID   97638\n",
       "...               ...     ...\n",
       "138447  bistochastize       1\n",
       "138448       WKTBASES       1\n",
       "138449         Toward       1\n",
       "138450         MICROS       1\n",
       "138451          itrin       1\n",
       "\n",
       "[138452 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34b12d-7240-4f32-a75a-808628c734f5",
   "metadata": {},
   "source": [
    "## Create a dictionary\n",
    "Let's try to use the nltk dictionary to determine if a word is a real english word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ff0b2a-2495-4a03-afc6-fb4481556663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:49.102488Z",
     "start_time": "2024-06-08T14:49:48.686664Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "# english_dictionary = set(words.words())\n",
    "\n",
    "# I will use a better dictionary: ENABLE (Enhanced North American Benchmark Lexicon)\n",
    "with open('SavedFiles/atebits.txt', 'r') as file:\n",
    "    words = file.read().splitlines()\n",
    "english_dictionary =  set(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632234ba-0962-489e-a1b6-f573dd0ae517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:49.123319Z",
     "start_time": "2024-06-08T14:49:49.113594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274926"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7801da0-29f7-4909-af63-cbf533182286",
   "metadata": {},
   "source": [
    "Let's see how many real words are in the total terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe8c658-ab9b-44fd-942d-f0ca27208677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:49.205769Z",
     "start_time": "2024-06-08T14:49:49.118416Z"
    }
   },
   "outputs": [],
   "source": [
    "def lookup_terms(term):\n",
    "    return term.lower() in english_dictionary\n",
    "\n",
    "def percentage_of_real_word_unique(df_word_frequency, lookup_func=lookup_terms):\n",
    "    # assuming that the df_word_frequency have \"terms\" column and \"count\" column\n",
    "    df_word_frequency['real_word'] = df_word_frequency['terms'].apply(lookup_func)\n",
    "    return df_word_frequency['real_word'].mean() * 100\n",
    "\n",
    "def percentage_of_real_word(df_word_frequency, lookup_func=lookup_terms):\n",
    "    # assuming that the df_word_frequency have \"terms\" column and \"count\" column\n",
    "    df_word_frequency['real_word'] = df_word_frequency['terms'].apply(lookup_func)\n",
    "    return (df_word_frequency[df_word_frequency['real_word'] == 1]['count'].sum() / df_word_frequency['count'].sum())*100\n",
    "\n",
    "def number_of_real_word_unique(df_word_frequency, lookup_func=lookup_terms):\n",
    "    # assuming that the df_word_frequency have \"terms\" column\n",
    "    df_word_frequency['real_word'] = df_word_frequency['terms'].apply(lookup_func)\n",
    "    return df_word_frequency['real_word'].sum()\n",
    "\n",
    "def number_of_real_word(df_word_frequency, lookup_func=lookup_terms):\n",
    "    # assuming that the df_word_frequency have \"terms\" column\n",
    "    df_word_frequency['real_word'] = df_word_frequency['terms'].apply(lookup_func)\n",
    "    return (df_word_frequency[df_word_frequency['real_word'] == 1]['count'].sum()) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cec5fe2-4667-4038-9e67-0bc4619fe189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:49.941476Z",
     "start_time": "2024-06-08T14:49:49.126215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Chinese, before tokenization: \n",
      "Number of real words:\n",
      "535400300\n",
      "Number of unique real words: \n",
      "25923\n",
      "Percentage of real words in corpus: \n",
      "72.6715592126284\n",
      "Percentage of unique real words in all unique real words: \n",
      "32.33866842978506\n",
      "======================================\n",
      "For English, before tokenization: \n",
      "Number of real words:\n",
      "536840000\n",
      "Number of unique real words: \n",
      "29707\n",
      "Percentage of real words in corpus: \n",
      "72.67408804064247\n",
      "Percentage of unique real words in all unique real words: \n",
      "28.432376558866036\n",
      "======================================\n",
      "For total, before tokenization: \n",
      "Number of real words:\n",
      "1072240300\n",
      "Number of unique real words: \n",
      "35476\n",
      "Percentage of real words in corpus: \n",
      "72.67282530236845\n",
      "Percentage of unique real words in all unique real words: \n",
      "25.623320717649438\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "for df_word_frequency, df_name in [(df_chinese_word_freq, 'Chinese'), (df_english_word_freq, 'English'), (df_word_freq, 'total')]:\n",
    "    print(f\"For {df_name}, before tokenization: \")\n",
    "    print(\"Number of real words:\")\n",
    "    print(number_of_real_word(df_word_frequency))\n",
    "    print(\"Number of unique real words: \")\n",
    "    print(number_of_real_word_unique(df_word_frequency))\n",
    "    print(\"Percentage of real words in corpus: \")\n",
    "    print(percentage_of_real_word(df_word_frequency))\n",
    "    print(\"Percentage of unique real words in all unique real words: \")\n",
    "    print(percentage_of_real_word_unique(df_word_frequency))\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ae446-be87-4060-8498-d255accdae91",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "I noticed that the dictionary determines that some conjugation of a word , such as \"names\" (plural), and \"expected\" (past tense) is not a word. Thanks to the almighty ChatGPT, we can use lemmatization of check if a word is a real word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152dc1c0-6339-421b-a58d-9f6cd13d5541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:50.687577Z",
     "start_time": "2024-06-08T14:49:49.941745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/samyiin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Check if a word is in the NLTK words corpus after lemmatization\n",
    "def is_english_word(word):\n",
    "    base_word = lemmatizer.lemmatize(word.lower())\n",
    "    return base_word in english_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaab027-84d6-49eb-81e5-834de636ca07",
   "metadata": {},
   "source": [
    "Why don't we change the original words in the df? Because we might need to analyze the use of plural and tenses (or other conjugations) later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9efe2df4-7fc9-4bc6-a0e7-ac50799de929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:49:56.066246Z",
     "start_time": "2024-06-08T14:49:50.688817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Chinese, after tokenization: \n",
      "Number of real words:\n",
      "534645700\n",
      "Number of unique real words: \n",
      "25971\n",
      "Percentage of real words in corpus: \n",
      "72.56913499175693\n",
      "Percentage of unique real words in all unique real words: \n",
      "32.398547922306356\n",
      "======================================\n",
      "For English, after tokenization: \n",
      "Number of real words:\n",
      "535744700\n",
      "Number of unique real words: \n",
      "29734\n",
      "Percentage of real words in corpus: \n",
      "72.52581308231055\n",
      "Percentage of unique real words in all unique real words: \n",
      "28.45821808332456\n",
      "======================================\n",
      "For total, after tokenization: \n",
      "Number of real words:\n",
      "1070390400\n",
      "Number of unique real words: \n",
      "35549\n",
      "Percentage of real words in corpus: \n",
      "72.54744532968243\n",
      "Percentage of unique real words in all unique real words: \n",
      "25.67604657209719\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "for df_word_frequency, df_name in [(df_chinese_word_freq, 'Chinese'), (df_english_word_freq, 'English'), (df_word_freq, 'total')]:\n",
    "    print(f\"For {df_name}, after tokenization: \")\n",
    "    print(\"Number of real words:\")\n",
    "    print(number_of_real_word(df_word_frequency, lookup_func=is_english_word))\n",
    "    print(\"Number of unique real words: \")\n",
    "    print(number_of_real_word_unique(df_word_frequency,lookup_func=is_english_word))\n",
    "    print(\"Percentage of real words in corpus: \")\n",
    "    print(percentage_of_real_word(df_word_frequency, lookup_func=is_english_word))\n",
    "    print(\"Percentage of unique real words in all unique real words: \")\n",
    "    print(percentage_of_real_word_unique(df_word_frequency, lookup_func=is_english_word))\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1758267-5841-4057-8d01-eeb26c08e8a3",
   "metadata": {},
   "source": [
    "Simpson Paradox?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b33926-cfc1-4898-9866-4cfb3f113917",
   "metadata": {},
   "source": [
    "## Single letter\n",
    "Try to get rid of singgle letters and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbaf817-fa02-4416-b3df-d2c964b47abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:00.588122Z",
     "start_time": "2024-06-08T14:49:56.087958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Chinese, after tokenization: \n",
      "Number of real words:\n",
      "534645700\n",
      "Number of unique real words: \n",
      "25971\n",
      "Percentage of real words in corpus: \n",
      "77.25939858424789\n",
      "Percentage of unique real words in all unique real words: \n",
      "32.41957832453282\n",
      "======================================\n",
      "For English, after tokenization: \n",
      "Number of real words:\n",
      "535744700\n",
      "Number of unique real words: \n",
      "29734\n",
      "Percentage of real words in corpus: \n",
      "76.38633116707206\n",
      "Percentage of unique real words in all unique real words: \n",
      "28.47238846702608\n",
      "======================================\n",
      "For total, after tokenization: \n",
      "Number of real words:\n",
      "1070390400\n",
      "Number of unique real words: \n",
      "35549\n",
      "Percentage of real words in corpus: \n",
      "76.81993615792668\n",
      "Percentage of unique real words in all unique real words: \n",
      "25.685693641618496\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "for df_word_frequency, df_name in [(df_chinese_word_freq, 'Chinese'), (df_english_word_freq, 'English'), (df_word_freq, 'total')]:\n",
    "    df_word_frequency = df_word_frequency[df_word_frequency['terms'].apply(lambda x: len(x) != 1)].copy()\n",
    "    print(f\"For {df_name}, after tokenization: \")\n",
    "    print(\"Number of real words:\")\n",
    "    print(number_of_real_word(df_word_frequency, lookup_func=is_english_word))\n",
    "    print(\"Number of unique real words: \")\n",
    "    print(number_of_real_word_unique(df_word_frequency,lookup_func=is_english_word))\n",
    "    print(\"Percentage of real words in corpus: \")\n",
    "    print(percentage_of_real_word(df_word_frequency, lookup_func=is_english_word))\n",
    "    print(\"Percentage of unique real words in all unique real words: \")\n",
    "    print(percentage_of_real_word_unique(df_word_frequency, lookup_func=is_english_word))\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a5b400-e6d9-421d-a4bb-9feeb0ee36f5",
   "metadata": {},
   "source": [
    "improved by just a little"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9e1e1-dd62-452b-a6f8-4acc2a7d9b54",
   "metadata": {},
   "source": [
    "## translate the word\n",
    "Since there is only 138k unique terms, around 100k unknown words, we can try to use LLM to predict the word. \n",
    "Each term might have more than one occurances in the corpus, for example, the term \"init\" occurs in 173722 names. What we will do here is we will just select one occurance, and use it as a context for the LLM to guess the original word of the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae4fc49-ae06-4e19-8133-a1d695053cef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:03:24.455764Z",
     "start_time": "2024-06-08T16:03:24.396549Z"
    }
   },
   "outputs": [],
   "source": [
    "# we need to save this result, because it takes about 2 mins to run: \n",
    "import joblib\n",
    "memory = joblib.Memory(location='CacheFunctionCalls', verbose=1)\n",
    "# but probelem is, everytime we restart kernal it reruns.... So we rewrite the decorator. \n",
    "def cache(memory, module, **mem_kwargs):\n",
    "    def cache_(f):\n",
    "        f.__module__ = module\n",
    "        f.__qualname__ = f.__name__\n",
    "        return memory.cache(f, **mem_kwargs)\n",
    "    return cache_\n",
    "\n",
    "# cache the function call\n",
    "@cache(memory, \"04_abbriviations\")\n",
    "def get_dic_terms_to_index():\n",
    "    # we will use df from outer scope\n",
    "    # create a mapping between a term to it's corresponding rows in df\n",
    "    dic_term_to_rows = df.explode(\"terms\").groupby('terms')['terms'].apply(lambda x: x.index.tolist()).to_dict()\n",
    "    return dic_term_to_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a75f4-c328-45de-be00-e2935b7d088d",
   "metadata": {},
   "source": [
    "Now we will take a row (randomly? For now we take the first occurance), and make that row the context of the term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7147a081-b44e-4b66-8b57-465d91298ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:17.420944Z",
     "start_time": "2024-06-08T14:50:00.689894Z"
    }
   },
   "outputs": [],
   "source": [
    "dic_term_to_rows = get_dic_terms_to_index()\n",
    "\n",
    "def get_context(term):\n",
    "    # we will use df from outer scope\n",
    "    # there should not be any key_error because we create the dic from the same df\n",
    "    row_number = dic_term_to_rows[term][0]\n",
    "    return df.iloc[row_number]['name']\n",
    "\n",
    "# for each word in df_word_freq, we will get the first occurance of it as the context\n",
    "df_word_freq['context'] = df_word_freq['terms'].apply(get_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edb7e-3218-43ab-82c9-27a8f85bd18c",
   "metadata": {},
   "source": [
    "## Query LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9081a2c6-741a-4fef-a2a7-15ebb86fffbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:17.423286Z",
     "start_time": "2024-06-08T14:50:17.417417Z"
    }
   },
   "outputs": [],
   "source": [
    "# give prompts\n",
    "first_prompt = \"\"\"I have a list of (term, variable_name) touples, the term appears in the variable_name, variable_name is extracted from python programs. \n",
    "The term could be an abbreviation, acronym of words, or concatenation of words, I need you to guess what the term represents.\n",
    "If you don't know what the term represents, then give -1 as your answer. \n",
    "Your output is linked to a computer program, so your output should be in format of a list of strings. Each string corresponds to a touple.\n",
    "Do you understand?\"\"\"\n",
    "first_respond = \"Yes, I fully understand and I will only respond with a string literal of python list. Please provide the list of (variable_names, term) tuples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7d2e6-1199-4a80-b16a-eff260159013",
   "metadata": {},
   "source": [
    "We will use the idea of In context learning/ n-shots learning: In-context Learning (ICL; Brown et al., 2020), n-shots learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1336f28b-6fa8-47a3-805d-a90fd05af0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:17.474597Z",
     "start_time": "2024-06-08T14:50:17.424787Z"
    }
   },
   "outputs": [],
   "source": [
    "example1 = '[(init,init),(x,x),(y,y),(a,a),(s,token_pretrained_s2G),(msg,http_error_msg),(d,d),(X,diffX),(n,n),(url,txt2img_url)]'\n",
    "answer1 = \"['initialization', '-1', '-1', '-1', '-1', 'message', '-1', 'X', '-1', 'Uniform Resource Locator']\"\n",
    "\n",
    "example2 = '[(c,c),(str,__str__),(b,b),(args,arrow_args),(f,delta_f),(config,config),(p,p),(num,num_classes),(dir,data_dir),(t,calc_gradient_t)]'\n",
    "answer2 = \"['-1', 'string', '-1', 'arguments', '-1', 'configuration', '-1', 'number', 'directory', '-1']\"\n",
    "\n",
    "example3 = '[(r,r),(m,m),(repr,__repr__),(i,start_i),(D,train_pretrained_s2D),(v,v_fps),(func,func),(df,df),(iter,__iter__),(params,extract_params)]'\n",
    "answer3 = \"['-1', '-1', 'representation', 'index', 'dimention', '-1', 'function', 'dataframe', 'iterator', 'parameters']\"\n",
    "\n",
    "example4 = '[(webinf,webinf_path),(pxtc,all_data_pca_pxtc),(datatore,expect_datatore_lookup),(preimported,preimported_locals),(camerasfile,camerasfile),(camdata,camdata),(imagesfile,imagesfile),(bistochastize,bistochastize),(WKTBASES,WKTBASES),(itrin,idx_itrin)]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c02eec5-581e-41d9-8e27-5d8a666ad7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:17.475804Z",
     "start_time": "2024-06-08T14:50:17.431Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_access_token(file_path=\"OpenAIAPIKey\"):\n",
    "    \"\"\"\n",
    "    Put your access token in an empty file named: OpenAIAPIKey under the same directory of this file (Assume running\n",
    "    this script from the same directory where it located\n",
    "    :param file_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        access_token = file.read().strip()\n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33363b3b-c23a-4a11-b0c9-c4dedeb45889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:17.944576Z",
     "start_time": "2024-06-08T14:50:17.440891Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import ast\n",
    "\n",
    "# get API key from a file (We could have put them in a env variable, I just don't want to bother\n",
    "API_KEY = read_access_token()\n",
    "\n",
    "def try_guess(str_list_term_name_tuples):\n",
    "    client = openai.OpenAI(api_key=API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "        # model = \"gpt-4o\",\n",
    "        model = \"gpt-3.5-turbo-0125\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": first_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": first_respond},\n",
    "            {\"role\": \"user\", \"content\": example1},\n",
    "            {\"role\": \"assistant\", \"content\": answer1},\n",
    "            {\"role\": \"user\", \"content\": example2},\n",
    "            {\"role\": \"assistant\", \"content\": answer2},\n",
    "            {\"role\": \"user\", \"content\": example3},\n",
    "            {\"role\": \"assistant\", \"content\": answer3},\n",
    "            {\"role\": \"user\", \"content\": str_list_term_name_tuples}])\n",
    "    return response\n",
    "    \n",
    "def accept_response(response):\n",
    "    res_stop = True\n",
    "    res_correct = True\n",
    "    # first check if the response is correct\n",
    "    if not response.choices[0].finish_reason == \"stop\":\n",
    "        res_stop = False\n",
    "    # now let's check if the response is correct\n",
    "    gpt_res = response.choices[0].message.content\n",
    "    try:\n",
    "        actual_list = ast.literal_eval(gpt_res)\n",
    "        # now check if the response have the same size as the input \n",
    "        # (assume there is 10: the test will fail if we are running that last iter)\n",
    "        if len(actual_list) != 10:\n",
    "            res_correct = False\n",
    "    except:\n",
    "        res_correct = False\n",
    "    return res_stop and res_correct\n",
    "    \n",
    "# we will cache the results\n",
    "@cache(memory, \"04_abbriviations\")\n",
    "def guess_abbreviation_by_bulk(str_list_term_name_tuples):\n",
    "    \"\"\"\n",
    "    : param: str_list_term_name_tuples: list of tuples as in examples above\n",
    "    \"\"\"\n",
    "    response = try_guess(str_list_term_name_tuples)\n",
    "    save_money_counter = 1\n",
    "    while not accept_response(response):\n",
    "        print(\"response not accepted\")\n",
    "        response = try_guess(str_list_term_name_tuples)\n",
    "        save_money_counter += 1\n",
    "        if save_money_counter > 10:\n",
    "            # so that the result is not cached\n",
    "            raise ValueError\n",
    "    # if it gets to this point then it's correct, ast.literal_eval will succeed\n",
    "    return ast.literal_eval(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e064351b-ddb6-4065-8ca7-811ad6c0af74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:17.956051Z",
     "start_time": "2024-06-08T14:50:17.946056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['web information',\n",
       " '-1',\n",
       " 'data store',\n",
       " 'pre-imported',\n",
       " 'cameras file',\n",
       " 'camera data',\n",
       " 'images file',\n",
       " 'bistochastize',\n",
       " 'WKT BASES',\n",
       " 'iteration index']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_abbreviation_by_bulk(example4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52955d8a-b3d0-4fa3-91e5-d7a2fca1b991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:18.485777Z",
     "start_time": "2024-06-08T14:50:17.953673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102903"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abbrev_words = df_word_freq[df_word_freq['real_word']==False][['terms', 'context']]\n",
    "\n",
    "len(df_abbrev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "398f665e-6d62-431b-b03c-70337742bace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:22.564533Z",
     "start_time": "2024-06-08T14:50:18.478589Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "df_abbrev_words = df_word_freq[df_word_freq['real_word']==False][['terms', 'context']]\n",
    "\n",
    "def pad_term_name_tuples(name_term_tuples):\n",
    "    '''\n",
    "    First of all, the name_term_tuples is actually term_name_tuples, terms is the first column, \n",
    "    Since the guess_abbreviation_by_bulk is taking a string of 10, when the input is not exactly 10, we will pad it. \n",
    "    '''\n",
    "    current_length = len(name_term_tuples)\n",
    "    target_length = 10\n",
    "    if current_length < target_length:\n",
    "        # Calculate the number of tuples to add\n",
    "        num_tuples_to_add = target_length - current_length\n",
    "        # Extend the list with (0, 0) tuples\n",
    "        name_term_tuples.extend(['(0, 0)'] * num_tuples_to_add)\n",
    "    return name_term_tuples\n",
    "\n",
    "\n",
    "def init_abbrev_map(df_abbrev_words):\n",
    "    for i in range((len(df_abbrev_words) // 10)+1):\n",
    "        start = i * 10\n",
    "        end = start + 10\n",
    "        # get the next bulk of abbreviated words\n",
    "        name_term_tuples = ['('+','.join([row[0],row[1]])+')' for row in df_abbrev_words[start:end].to_numpy()]\n",
    "        name_term_tuples = pad_term_name_tuples(name_term_tuples)\n",
    "        str_list_term_name_tuples = '[' + ','.join(name_term_tuples) + ']'\n",
    "        # get respond from LLM: we will cache the result, so there is no need to record the output\n",
    "        guess_abbreviation_by_bulk(str_list_term_name_tuples)\n",
    "\n",
    "import time\n",
    "def init_abbrev_map_robust(df_abbrev_words):\n",
    "    '''This function is robust to internet fail and ValueError from ChatGPT'''\n",
    "    weird_inputs = []\n",
    "    while True:\n",
    "            try:\n",
    "                init_abbrev_map(df_abbrev_words[:])\n",
    "                break  # Exit the loop if function executes successfully\n",
    "            except ValueError as e:\n",
    "                weird_inputs.append(str_list_term_name_tuples)\n",
    "                print(\"found weird input\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "                print(\"Retrying function...\")\n",
    "                time.sleep(50)  # Wait for a while before retrying\n",
    "            \n",
    "    return weird_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b51447c6-f181-42ce-a808-fb0313e465a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_err_outputs():\n",
    "    \"\"\"\n",
    "    In  my previous run I forgot to check if the result from chat gpt is of the right length, \n",
    "    So I am fixing that error here, now I have fixed the error, this function should only be ran once\n",
    "    \"\"\"\n",
    "    for i in range(len(df_abbrev_words) // 10):\n",
    "        start = i * 10\n",
    "        end = start + 10\n",
    "        # get the next bulk of abbreviated words\n",
    "        name_term_tuples = ['('+','.join([row[0],row[1]])+')' for row in df_abbrev_words[start:end].to_numpy()]\n",
    "        str_list_term_name_tuples = '[' + ','.join(name_term_tuples) + ']'\n",
    "        # get respond from LLM: we will cache the result, so there is no need to record the output\n",
    "        list_abbrev_meaning = guess_abbreviation_by_bulk(str_list_term_name_tuples)\n",
    "        if len(list_abbrev_meaning) != 10:\n",
    "            # clear the caches\n",
    "            result = guess_abbreviation_by_bulk.call_and_shelve(str_list_term_name_tuples)\n",
    "            result.clear()\n",
    "        \n",
    "# clear_err_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4841d-047d-45c8-9c84-c30b4dd96f88",
   "metadata": {},
   "source": [
    "Retrospect here: taking a string representation of a list as input is really a bad design. If I take input in as a set, then if what's passed in is a subset, it would not rerun everything. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa84df-c5d9-40dd-b777-d1d6cd775c7d",
   "metadata": {},
   "source": [
    "### Abbrive map initialzation (0 iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "359847af-7ff1-45a1-862a-55331da24aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:04:05.553407Z",
     "start_time": "2024-06-08T16:03:54.454291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>name</th>\n",
       "      <th>abbrev_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init</td>\n",
       "      <td>init</td>\n",
       "      <td>initialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s</td>\n",
       "      <td>token_pretrained_s2G</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102898</th>\n",
       "      <td>camdata</td>\n",
       "      <td>camdata</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102899</th>\n",
       "      <td>imagesfile</td>\n",
       "      <td>imagesfile</td>\n",
       "      <td>images file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102900</th>\n",
       "      <td>bistochastize</td>\n",
       "      <td>bistochastize</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102901</th>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>well-known text bases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102902</th>\n",
       "      <td>itrin</td>\n",
       "      <td>idx_itrin</td>\n",
       "      <td>iteration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102903 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term                  name         abbrev_meaning\n",
       "0                init                  init         initialization\n",
       "1                   x                     x                     -1\n",
       "2                   y                     y                     -1\n",
       "3                   a                     a                     -1\n",
       "4                   s  token_pretrained_s2G                     -1\n",
       "...               ...                   ...                    ...\n",
       "102898        camdata               camdata                     -1\n",
       "102899     imagesfile            imagesfile            images file\n",
       "102900  bistochastize         bistochastize                     -1\n",
       "102901       WKTBASES              WKTBASES  well-known text bases\n",
       "102902          itrin             idx_itrin              iteration\n",
       "\n",
       "[102903 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "df_abbrev_words = df_word_freq[df_word_freq['real_word']==False][['terms', 'context']]\n",
    "    \n",
    "def get_abbrev_map(df_abbrev_words, weird_inputs):\n",
    "    list_all_terms = []\n",
    "    list_all_names = []\n",
    "    list_all_abbrev_meaning = []\n",
    "    for i in range((len(df_abbrev_words) // 10)+1):\n",
    "        start = i * 10\n",
    "        end = start + 10\n",
    "        # get the next bulk of abbreviated words\n",
    "        name_term_tuples = ['('+','.join([row[0],row[1]])+')' for row in df_abbrev_words[start:end].to_numpy()]\n",
    "        name_term_tuples = pad_term_name_tuples(name_term_tuples)\n",
    "        str_list_term_name_tuples = '[' + ','.join(name_term_tuples) + ']'\n",
    "        # during init there are weird inputs that ChatGPT just cannot get it right, so we will not rerun that. \n",
    "        if str_list_term_name_tuples in weird_inputs:\n",
    "            list_abbrev_meaning = ['-1'] * 10\n",
    "        else:\n",
    "            list_abbrev_meaning = guess_abbreviation_by_bulk(str_list_term_name_tuples)\n",
    "        list_all_abbrev_meaning.extend(list_abbrev_meaning)        \n",
    "        # create the list of names\n",
    "        list_terms = [row[0] for row in df_abbrev_words[start:end].to_numpy()]\n",
    "        list_all_terms.extend(list_terms)\n",
    "        list_names = [row[1] for row in df_abbrev_words[start:end].to_numpy()]\n",
    "        list_all_names.extend(list_names) \n",
    "    df_abbrev_map = pd.DataFrame({\n",
    "    'term': list_all_terms,\n",
    "    'name': list_all_names,\n",
    "    'abbrev_meaning': list_all_abbrev_meaning[: len(df_abbrev_words)]\n",
    "    })\n",
    "    return df_abbrev_map\n",
    "\n",
    "weird_inputs = init_abbrev_map_robust(df_abbrev_words)\n",
    "df_abbrev_map = get_abbrev_map(df_abbrev_words, weird_inputs)\n",
    "df_abbrev_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96b021-2e8f-4412-b303-a7cbe0afcfb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:50:27.213900Z",
     "start_time": "2024-06-08T14:50:26.985312Z"
    }
   },
   "source": [
    "Something that is definitely worth noting is that there are certain inputs that is just hard for chatGPT to find answer for. And the weird thing here is that they are usually not the long ones or the complex ones. Characterizing them may leads to so good results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5932f951-25d2-43fd-bb2c-3ab3f94a9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62315 102903\n"
     ]
    }
   ],
   "source": [
    "'''There is something I forgot to check, and now it's too late to change because I have cached all the results, \n",
    "So I have to change it here now: the return of chat GPT might be '-1' (str) or -1 (int). '''\n",
    "def convert_neg_one(value):\n",
    "    if isinstance(value, str) and value.strip() == '-1':\n",
    "        return -1\n",
    "    return value\n",
    "df_abbrev_map['abbrev_meaning'] = df_abbrev_map['abbrev_meaning'].apply(convert_neg_one)\n",
    "\n",
    "df_abbrev_map_success = df_abbrev_map[df_abbrev_map['abbrev_meaning']!= -1]\n",
    "print(df_abbrev_map_success.shape[0], df_abbrev_map.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae4758-2073-47fb-8f7f-fe269e12b7bd",
   "metadata": {},
   "source": [
    "### Abbrieve map iteration\n",
    "There are more abbreviations that ChatGPT should have recognized, So in order to find as many as possible, I would like to try a different approach. There are two things to balance here: I don't want chatGPT to recognize it should't know, for example, recognize a as apple. On the other hand, I want chatGPT to recognize things it should know, like, webinf is web information.  \n",
    "I can't think of any good solution at this point, what I can think of, is to run it multiple times one different inputs, iteratively, and hoping that it can recognize more and more...  \n",
    "Besides, I will change the prompt, reduce the example where there is '-1' in the results, hoping that ChatGPT will not recognize it as a pattern. Because in my examples there are 14/30 '-1', and in the result, there are almost the same percenatge of '-1', so I suspect that.  \n",
    "2024.06.06 I observe that this is improving the results, but it is too \"confidient\" in a sense that would predict a as alpha. Let me first use this method, and then determine should I keep the single letter's interpretation: because sometimes it would also predict s to sequence, which is chorrect under context token_pretrained_s2G.  \n",
    "Now, finally, we ran 40k and we left with 4k unknown. So I guess providing new context deos help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dbba083-4f88-4388-b727-86434a6d1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_examples(start, df_abbrev_map):\n",
    "    '''In the first iteration, I have to type out examples manually, but I get some results, \n",
    "    so now I will generate examples by what I already have, the manual examples will be overritten at this point.'''\n",
    "    end = start + 10\n",
    "    df_abbrev_map_success = df_abbrev_map[df_abbrev_map['abbrev_meaning']!= '-1']\n",
    "    name_term_tuples = ['('+','.join([row[0],row[1]])+')' for row in df_abbrev_map[start:end].to_numpy()]\n",
    "    str_list_term_name_tuples = '[' + ','.join(name_term_tuples) + ']'\n",
    "    results = [\"'\" + row[2] + \"'\" for row in df_abbrev_map_success[start:end].to_numpy()]\n",
    "    str_list_results = '[' + ','.join(results) + ']'\n",
    "    return str_list_term_name_tuples, str_list_results\n",
    "\n",
    "# this will overrite the examples above\n",
    "example1, answer1 = generating_examples(0, df_abbrev_map_success)\n",
    "example2, answer2 = generating_examples(10, df_abbrev_map_success)\n",
    "example3, answer3 = generating_examples(20, df_abbrev_map_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c87528be-349f-4254-b1a9-132e9761004d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[(i,start_i),(D,train_pretrained_s2D),(func,func),(df,df),(iter,__iter__),(params,extract_params),(obj,nmap_obj),(img,img),(val,val),(idx,idx)]',\n",
       " \"['index','dimension','function','dataframe','iterator','parameters','object','image','value','index']\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example2, answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16391314-3f2c-44e6-a676-9033a6f3f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abbrev_meaning(row):\n",
    "    '''assuming that the row have 'abbrev_meaning' and 'abbrev_meaning_to_add' '''\n",
    "    if row['abbrev_meaning'] == -1 :\n",
    "        row['abbrev_meaning'] = row['abbrev_meaning_to_add']\n",
    "    return row\n",
    "\n",
    "def iterative_fill_in_abbriv_map(df_abbrev_map, n_iter):\n",
    "    for i in range(n_iter):\n",
    "        df_abbrev_map_fail = df_abbrev_map[df_abbrev_map['abbrev_meaning'] == -1]\n",
    "        weird_inputs = init_abbrev_map_robust(df_abbrev_map_fail)\n",
    "        df_abbrev_map_mixed = get_abbrev_map(df_abbrev_map_fail, weird_inputs)\n",
    "        \n",
    "        # merge the new result with the old result -- df_abbrev_map\n",
    "        df_abbrev_map = pd.merge(df_abbrev_map, df_abbrev_map_mixed, how='left', on=['term', 'name'], suffixes=('', '_to_add'))\n",
    "        df_abbrev_map = df_abbrev_map.apply(add_abbrev_meaning, axis=1)\n",
    "        # convert '-1' to -1, trim the columns\n",
    "        df_abbrev_map = df_abbrev_map[['term', 'name', 'abbrev_meaning']]\n",
    "        df_abbrev_map['abbrev_meaning'] = df_abbrev_map['abbrev_meaning'].apply(convert_neg_one)\n",
    "    return df_abbrev_map\n",
    "\n",
    "# don't run this function twice, it will continue running\n",
    "\n",
    "df_abbrev_map = iterative_fill_in_abbriv_map(df_abbrev_map, n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57f2ad79-c71a-4fb5-a010-89b3263984fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>name</th>\n",
       "      <th>abbrev_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init</td>\n",
       "      <td>init</td>\n",
       "      <td>initialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s</td>\n",
       "      <td>token_pretrained_s2G</td>\n",
       "      <td>token pretrained sequence to graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102898</th>\n",
       "      <td>camdata</td>\n",
       "      <td>camdata</td>\n",
       "      <td>camera data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102899</th>\n",
       "      <td>imagesfile</td>\n",
       "      <td>imagesfile</td>\n",
       "      <td>images file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102900</th>\n",
       "      <td>bistochastize</td>\n",
       "      <td>bistochastize</td>\n",
       "      <td>convert to bistochastic matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102901</th>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>well-known text bases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102902</th>\n",
       "      <td>itrin</td>\n",
       "      <td>idx_itrin</td>\n",
       "      <td>iteration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102903 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term                  name  \\\n",
       "0                init                  init   \n",
       "1                   x                     x   \n",
       "2                   y                     y   \n",
       "3                   a                     a   \n",
       "4                   s  token_pretrained_s2G   \n",
       "...               ...                   ...   \n",
       "102898        camdata               camdata   \n",
       "102899     imagesfile            imagesfile   \n",
       "102900  bistochastize         bistochastize   \n",
       "102901       WKTBASES              WKTBASES   \n",
       "102902          itrin             idx_itrin   \n",
       "\n",
       "                            abbrev_meaning  \n",
       "0                           initialization  \n",
       "1                                        x  \n",
       "2                                        y  \n",
       "3                                    alpha  \n",
       "4       token pretrained sequence to graph  \n",
       "...                                    ...  \n",
       "102898                         camera data  \n",
       "102899                         images file  \n",
       "102900      convert to bistochastic matrix  \n",
       "102901               well-known text bases  \n",
       "102902                           iteration  \n",
       "\n",
       "[102903 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abbrev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "214ca45f-00e6-4ba7-bc3b-7868f0916893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>name</th>\n",
       "      <th>abbrev_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>nokey</td>\n",
       "      <td>test_read_nokey</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22080</th>\n",
       "      <td>truncexpon</td>\n",
       "      <td>truncexpon_gen</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22150</th>\n",
       "      <td>Renderables</td>\n",
       "      <td>Renderables</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22223</th>\n",
       "      <td>cfm</td>\n",
       "      <td>cfm</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25508</th>\n",
       "      <td>funm</td>\n",
       "      <td>funm</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101195</th>\n",
       "      <td>univgw</td>\n",
       "      <td>_univgw</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101197</th>\n",
       "      <td>argcounts</td>\n",
       "      <td>vtbl_argcounts</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101530</th>\n",
       "      <td>MASKEDSTRUCT</td>\n",
       "      <td>_WIN32MASKEDSTRUCT</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101532</th>\n",
       "      <td>lvc</td>\n",
       "      <td>lvc</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101533</th>\n",
       "      <td>hklm</td>\n",
       "      <td>hklm</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                term                name abbrev_meaning\n",
       "17596          nokey     test_read_nokey             -1\n",
       "22080     truncexpon      truncexpon_gen             -1\n",
       "22150    Renderables         Renderables             -1\n",
       "22223            cfm                 cfm             -1\n",
       "25508           funm                funm             -1\n",
       "...              ...                 ...            ...\n",
       "101195        univgw             _univgw             -1\n",
       "101197     argcounts      vtbl_argcounts             -1\n",
       "101530  MASKEDSTRUCT  _WIN32MASKEDSTRUCT             -1\n",
       "101532           lvc                 lvc             -1\n",
       "101533          hklm                hklm             -1\n",
       "\n",
       "[277 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abbrev_map_fail = df_abbrev_map[df_abbrev_map['abbrev_meaning'] == -1]\n",
    "df_abbrev_map_fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514df0b-ce84-4105-93c2-edd3b02a4f2e",
   "metadata": {},
   "source": [
    "after 0 iter: 40k  \n",
    "after 1 iter: 4k  \n",
    "after 2 iter: 800  \n",
    "after 3 iter: 277  \n",
    "There are several problems with this approach:\n",
    "1. predicting \"too much\": a -> alpha, h -> height\n",
    "2. There is no mapping for each instance: we selected just one instance for each abbreviation.\n",
    "3. Didn't recognize: MASKEDSTRUCT = masked sctruct\n",
    "4. Output error: (s,token_pretrained_s2G) -> \"token pretrained sequence to graph\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675693e3-2a4b-44da-a92b-fc8b6afc9329",
   "metadata": {},
   "source": [
    "### Store results to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f63254be-b44c-4c6c-9cfc-519f9cefb281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>name</th>\n",
       "      <th>abbrev_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init</td>\n",
       "      <td>init</td>\n",
       "      <td>initialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s</td>\n",
       "      <td>token_pretrained_s2G</td>\n",
       "      <td>token pretrained sequence to graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102898</th>\n",
       "      <td>camdata</td>\n",
       "      <td>camdata</td>\n",
       "      <td>camera data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102899</th>\n",
       "      <td>imagesfile</td>\n",
       "      <td>imagesfile</td>\n",
       "      <td>images file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102900</th>\n",
       "      <td>bistochastize</td>\n",
       "      <td>bistochastize</td>\n",
       "      <td>convert to bistochastic matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102901</th>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>well-known text bases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102902</th>\n",
       "      <td>itrin</td>\n",
       "      <td>idx_itrin</td>\n",
       "      <td>iteration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102903 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term                  name  \\\n",
       "0                init                  init   \n",
       "1                   x                     x   \n",
       "2                   y                     y   \n",
       "3                   a                     a   \n",
       "4                   s  token_pretrained_s2G   \n",
       "...               ...                   ...   \n",
       "102898        camdata               camdata   \n",
       "102899     imagesfile            imagesfile   \n",
       "102900  bistochastize         bistochastize   \n",
       "102901       WKTBASES              WKTBASES   \n",
       "102902          itrin             idx_itrin   \n",
       "\n",
       "                            abbrev_meaning  \n",
       "0                           initialization  \n",
       "1                                        x  \n",
       "2                                        y  \n",
       "3                                    alpha  \n",
       "4       token pretrained sequence to graph  \n",
       "...                                    ...  \n",
       "102898                         camera data  \n",
       "102899                         images file  \n",
       "102900      convert to bistochastic matrix  \n",
       "102901               well-known text bases  \n",
       "102902                           iteration  \n",
       "\n",
       "[102903 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abbrev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0746161c-5a3e-450d-8a4c-8bf3bc8096ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>name</th>\n",
       "      <th>abbrev_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init</td>\n",
       "      <td>init</td>\n",
       "      <td>initialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s</td>\n",
       "      <td>token_pretrained_s2G</td>\n",
       "      <td>token pretrained sequence to graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102898</th>\n",
       "      <td>camdata</td>\n",
       "      <td>camdata</td>\n",
       "      <td>camera data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102899</th>\n",
       "      <td>imagesfile</td>\n",
       "      <td>imagesfile</td>\n",
       "      <td>images file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102900</th>\n",
       "      <td>bistochastize</td>\n",
       "      <td>bistochastize</td>\n",
       "      <td>convert to bistochastic matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102901</th>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>well-known text bases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102902</th>\n",
       "      <td>itrin</td>\n",
       "      <td>idx_itrin</td>\n",
       "      <td>iteration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102903 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term                  name  \\\n",
       "0                init                  init   \n",
       "1                   x                     x   \n",
       "2                   y                     y   \n",
       "3                   a                     a   \n",
       "4                   s  token_pretrained_s2G   \n",
       "...               ...                   ...   \n",
       "102898        camdata               camdata   \n",
       "102899     imagesfile            imagesfile   \n",
       "102900  bistochastize         bistochastize   \n",
       "102901       WKTBASES              WKTBASES   \n",
       "102902          itrin             idx_itrin   \n",
       "\n",
       "                            abbrev_meaning  \n",
       "0                           initialization  \n",
       "1                                        x  \n",
       "2                                        y  \n",
       "3                                    alpha  \n",
       "4       token pretrained sequence to graph  \n",
       "...                                    ...  \n",
       "102898                         camera data  \n",
       "102899                         images file  \n",
       "102900      convert to bistochastic matrix  \n",
       "102901               well-known text bases  \n",
       "102902                           iteration  \n",
       "\n",
       "[102903 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abbrev_map = df_abbrev_map.astype(str)\n",
    "df_abbrev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4794b7f1-1423-4fc9-8b25-9302d2a548ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev_table = \"AbbreviationMap\"\n",
    "# sqlite connect is also relative path relative to the folder running the script.\n",
    "conn = sqlite3.connect('data.db')\n",
    "# store results to the table\n",
    "df_abbrev_map.to_sql(abbrev_table, conn, index=False, if_exists='replace')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2db9d6e-f9b0-4dbb-a745-426edfb8cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NameTable', 'AbbreviationMap']\n"
     ]
    }
   ],
   "source": [
    "# sqlite connect is also relative path relative to the folder running the script.\n",
    "conn = sqlite3.connect('data.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "table_names = cursor.fetchall()\n",
    "print([name[0] for name in table_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b48220b-0af1-47f1-a51f-17a0d733f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('data.db')\n",
    "# abbrev_table = \"AbbreviationMap\"\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(f\"DROP TABLE IF EXISTS {abbrev_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc1ff533-9ae8-48be-9a40-c66b9197813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data.db')\n",
    "query = f\"SELECT * FROM {abbrev_table}\"\n",
    "df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "950c9b50-e5c5-425b-9d8f-14c6672c9d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>name</th>\n",
       "      <th>abbrev_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init</td>\n",
       "      <td>init</td>\n",
       "      <td>initialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s</td>\n",
       "      <td>token_pretrained_s2G</td>\n",
       "      <td>token pretrained sequence to graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102898</th>\n",
       "      <td>camdata</td>\n",
       "      <td>camdata</td>\n",
       "      <td>camera data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102899</th>\n",
       "      <td>imagesfile</td>\n",
       "      <td>imagesfile</td>\n",
       "      <td>images file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102900</th>\n",
       "      <td>bistochastize</td>\n",
       "      <td>bistochastize</td>\n",
       "      <td>convert to bistochastic matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102901</th>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>WKTBASES</td>\n",
       "      <td>well-known text bases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102902</th>\n",
       "      <td>itrin</td>\n",
       "      <td>idx_itrin</td>\n",
       "      <td>iteration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102903 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term                  name  \\\n",
       "0                init                  init   \n",
       "1                   x                     x   \n",
       "2                   y                     y   \n",
       "3                   a                     a   \n",
       "4                   s  token_pretrained_s2G   \n",
       "...               ...                   ...   \n",
       "102898        camdata               camdata   \n",
       "102899     imagesfile            imagesfile   \n",
       "102900  bistochastize         bistochastize   \n",
       "102901       WKTBASES              WKTBASES   \n",
       "102902          itrin             idx_itrin   \n",
       "\n",
       "                            abbrev_meaning  \n",
       "0                           initialization  \n",
       "1                                        x  \n",
       "2                                        y  \n",
       "3                                    alpha  \n",
       "4       token pretrained sequence to graph  \n",
       "...                                    ...  \n",
       "102898                         camera data  \n",
       "102899                         images file  \n",
       "102900      convert to bistochastic matrix  \n",
       "102901               well-known text bases  \n",
       "102902                           iteration  \n",
       "\n",
       "[102903 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d04ba-9dd5-414d-84d0-0d96c7275f01",
   "metadata": {},
   "source": [
    "Later might need to mannually go through some of the cases......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
