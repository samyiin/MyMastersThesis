{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2084b70b5b67b5d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Description\n",
    "This file is used to check the distribution of words used in each project, and then compare the results in various ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0344e8112e8c68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Read Files\n",
    "First we will read the result from previous work, in which we get python files from internet, parse them to get all the variable and function names (and their scope, might be useful later), and then we parse the variable and function names into terms (in a primitive way). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:09:03.518652Z",
     "start_time": "2024-05-19T07:07:07.601262Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mterms\u001b[38;5;241m.\u001b[39mapply(json\u001b[38;5;241m.\u001b[39mloads)\n",
      "File \u001b[0;32m~/Projects/MyMastersThesis/venv/lib/python3.12/site-packages/pandas/io/sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/MyMastersThesis/venv/lib/python3.12/site-packages/pandas/io/sql.py:2751\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_iterator(\n\u001b[1;32m   2741\u001b[0m         cursor,\n\u001b[1;32m   2742\u001b[0m         chunksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2748\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   2749\u001b[0m     )\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2751\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_as_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2752\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2754\u001b[0m     frame \u001b[38;5;241m=\u001b[39m _wrap_result(\n\u001b[1;32m   2755\u001b[0m         data,\n\u001b[1;32m   2756\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2761\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   2762\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/MyMastersThesis/venv/lib/python3.12/site-packages/pandas/io/sql.py:2766\u001b[0m, in \u001b[0;36mSQLiteDatabase._fetchall_as_list\u001b[0;34m(self, cur)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetchall_as_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, cur):\n\u001b[0;32m-> 2766\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2768\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(result)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "name_table = \"NameTable\"\n",
    "conn = sqlite3.connect('data.db')\n",
    "query = f\"SELECT * FROM {name_table}\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "import json\n",
    "df['terms'] = df.terms.apply(json.loads)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58f0a04184a00a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:09:03.927189Z",
     "start_time": "2024-05-19T07:09:03.488713Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('words')\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9a6910d307de8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Average Length of Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4b5bcd57d8d43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Maybe we should save this back to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9667455272028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Length by Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5175ad753a8518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:57:08.121396Z",
     "start_time": "2024-05-19T07:56:27.823940Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def count_length_by_letter(name):\n",
    "    # we don't count numbers and underscore\n",
    "    pattern = r'[0-9_]'\n",
    "    name = re.sub(pattern, '', name)\n",
    "    return len(name)\n",
    "\n",
    "df[\"lengthByLetter\"] = df['name'].apply(count_length_by_letter)\n",
    "df['lengthByWord'] = df['terms'].apply(len)\n",
    "df = df[(df['lengthByLetter'] > 0) & (df['lengthByWord'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb351c04cd107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:00:49.974564Z",
     "start_time": "2024-05-19T08:00:33.028527Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average Length (by letter) of Chinese-speaking Programmers are: \")\n",
    "print(df[df['authorLocation'] == 'China']['lengthByLetter'].mean())\n",
    "print(\"Average Length (by letter) of English-speaking Programmers are: \")\n",
    "print(df[df['authorLocation'] == 'USA']['lengthByLetter'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5248cc488ff94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:00:04.547470Z",
     "start_time": "2024-05-19T07:59:46.561457Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's add the brown's data. Frequency is calculated by (number of word with length k) / (number of words) for each k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370f3bf66db26eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:00:59.704516Z",
     "start_time": "2024-05-19T08:00:55.813008Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# Get the words from the Brown Corpus\n",
    "brown_words = brown.words()\n",
    "# get length of brown words\n",
    "total_words = len(brown_words)\n",
    "# Calculate word frequencies\n",
    "dic_brown_word_freq = nltk.FreqDist(brown_words)\n",
    "# normalize the word frequency\n",
    "dic_brown_word_freq = {word: freq / total_words for word, freq in dic_brown_word_freq.items()}\n",
    "# put the normalized word frequency into pandas series \n",
    "df_brown_word_freq = pd.DataFrame.from_dict(dic_brown_word_freq, orient='index').reset_index()\n",
    "df_brown_word_freq.columns = ['word', 'proportion']\n",
    "# calculate length of letters frequency \n",
    "df_brown_word_freq['lengthByLetter'] = df_brown_word_freq['word'].apply(len)\n",
    "df_brown_letter_len_freq = df_brown_word_freq.groupby('lengthByLetter')['proportion'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648dd4aabefa877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:01:16.757357Z",
     "start_time": "2024-05-19T08:01:04.469877Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_chinese_letter_len_freq = df[df['authorLocation'] == 'China']['lengthByLetter'].value_counts(normalize=True).sort_index()\n",
    "df_english_letter_len_freq = df[df['authorLocation'] == 'USA']['lengthByLetter'].value_counts(normalize=True).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838dd642d6e4df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:01:19.953943Z",
     "start_time": "2024-05-19T08:01:18.953804Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_brown_letter_len_freq.plot(kind='bar', color='green', label='Brown')\n",
    "df_chinese_letter_len_freq.plot(kind='bar', color='blue', label='China')\n",
    "df_english_letter_len_freq.plot(kind='bar', color='red', label='USA')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Normalized Frequency of Numbers')\n",
    "plt.legend()\n",
    "# show less x labels\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(7))  # Adjust '5' to display the desired number of ticks\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43114b23d3a9c0f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The cdf version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cb408f64ae266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:07:24.308711Z",
     "start_time": "2024-05-19T08:07:24.125616Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# df_brown_letter_len_freq.cumsum().plot(kind='bar', color='green', label='Brown')\n",
    "df_chinese_letter_len_freq.cumsum().plot(kind='bar', color='blue', label='China')\n",
    "df_english_letter_len_freq.cumsum().plot(kind='bar', color='red', label='USA')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Normalized Frequency of Numbers')\n",
    "plt.legend()\n",
    "# show less x labels\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(7))  # Adjust '5' to display the desired number of ticks\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526de6bd2121b09b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Plot them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c76b3caaf9126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:02:08.909727Z",
     "start_time": "2024-05-19T08:01:29.648201Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "df_brown_letter_len_freq.plot(kind='bar', color='green', label='Brown', ax=axs[0])\n",
    "df_chinese_letter_len_freq.plot(kind='bar', color='blue', label='China', ax=axs[1])\n",
    "df_english_letter_len_freq.plot(kind='bar', color='red', label='USA', alpha=0.7, position=0.5, ax=axs[2])\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    # show legend\n",
    "    ax.legend()\n",
    "    # show less x labels\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(7))  # Adjust '5' to display the desired number of ticks\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd7d589406a4f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:02:13.164978Z",
     "start_time": "2024-05-19T08:02:13.141711Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_brown_letter_len_freq = df_brown_letter_len_freq.cumsum()\n",
    "df_brown_letter_len_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a88fe3adb0af9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's see the CDF version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246447e7092baf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:08:54.565405Z",
     "start_time": "2024-05-19T08:08:54.083540Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "df_brown_letter_len_freq.cumsum().plot(kind='bar', color='green', label='Brown', ax=axs[0])\n",
    "df_chinese_letter_len_freq.cumsum().plot(kind='bar', color='blue', label='China', ax=axs[1])\n",
    "df_english_letter_len_freq.cumsum().plot(kind='bar', color='red', label='USA', alpha=0.7, position=0.5, ax=axs[2])\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    # show legend\n",
    "    ax.legend()\n",
    "    # show less x labels\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(7))  # Adjust '5' to display the desired number of ticks\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326367a33607b633",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Length by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccae0ff95e7193",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:02:36.298213Z",
     "start_time": "2024-05-19T08:02:21.298863Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average Length (by word) of Chinese-speaking Programmers are: \")\n",
    "print(df[df['authorLocation'] == 'China']['lengthByWord'].mean())\n",
    "print(\"Average Length (by word) of English-speaking Programmers are: \")\n",
    "print(df[df['authorLocation'] == 'USA']['lengthByWord'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930b1fda78cf65d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:02:53.628960Z",
     "start_time": "2024-05-19T08:02:40.442373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_chinese_word_len_freq = df[df['authorLocation'] == 'China']['lengthByWord'].value_counts(normalize=True).sort_index()\n",
    "df_english_word_len_freq = df[df['authorLocation'] == 'USA']['lengthByWord'].value_counts(normalize=True).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3f51fc4152a4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:02:55.042778Z",
     "start_time": "2024-05-19T08:02:54.812971Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_chinese_word_len_freq.plot(kind='bar', color='blue', label='China')\n",
    "df_english_word_len_freq.plot(kind='bar', color='red', label='USA', alpha=0.7, position=0.5)\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Normalized Frequency of Numbers')\n",
    "plt.legend()\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb88c1862f12629",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "the cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17c94dc678f2be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:09:59.995848Z",
     "start_time": "2024-05-19T08:09:59.812562Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_chinese_word_len_freq.cumsum().plot(kind='bar', color='blue', label='China')\n",
    "df_english_word_len_freq.cumsum().plot(kind='bar', color='red', label='USA', alpha=0.7, position=0.5)\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Normalized Frequency of Numbers')\n",
    "plt.legend()\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917be8d9854511f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Length by letter X by word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c871fa7e97130",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here we are trying to recreate Nitssan's Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f303b59849f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:10.122263Z",
     "start_time": "2024-05-19T08:03:03.387142Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_letterXword(df_language):\n",
    "    # assume that the dataframe have both lengthByLetter and lengthByWord columns. \n",
    "    df_letterXword_len_freq = df_language.groupby(['lengthByLetter', 'lengthByWord']).size().reset_index(name='count')\n",
    "    # pivot the dataframe\n",
    "    pivot_df = df_letterXword_len_freq.pivot(index='lengthByLetter', columns='lengthByWord', values='count').fillna(0)\n",
    "    # plot the dataframe\n",
    "    pivot_df.plot(kind='bar', stacked=True)\n",
    "    plt.xlabel('Length by Letter')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Stacked Bar Plot of Length by Letter with Length by Word')\n",
    "    plt.legend(title='Length by Word')\n",
    "    plt.show()\n",
    "\n",
    "df_chinese = df[df['authorLocation'] == 'China']\n",
    "\n",
    "plot_letterXword(df_chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cceba12a4f4bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:20.020219Z",
     "start_time": "2024-05-19T08:03:14.982664Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_english = df[df['authorLocation'] == 'USA']\n",
    "plot_letterXword(df_english)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521010c69b5b2df8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T07:16:52.480429Z",
     "start_time": "2024-04-30T07:16:52.071124Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Most Common Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983270046875823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T07:16:44.979291Z",
     "start_time": "2024-04-30T07:16:43.566505Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's see the number of occurrences for each word. I mean, let's first see the top 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2b790947cb610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:36.370173Z",
     "start_time": "2024-05-19T08:03:25.550537Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "flattened_words_chinese = df[df['authorLocation'] == 'China']['terms'].explode()\n",
    "# Count the occurrences of each word\n",
    "word_counts_chinese = flattened_words_chinese.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31011d3cb643ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:37.610103Z",
     "start_time": "2024-05-19T08:03:37.570079Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "word_counts_chinese.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218743db58018dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:50.685044Z",
     "start_time": "2024-05-19T08:03:40.460828Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "flattened_words_english = df[df['authorLocation'] == 'USA']['terms'].explode()\n",
    "# Count the occurrences of each word\n",
    "word_counts_english = flattened_words_english.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d2e0a6adf5c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:51.816634Z",
     "start_time": "2024-05-19T08:03:51.788856Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "word_counts_english.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c49e107fdeba59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "todo: single letter names - states - are they the same ones? Need to wait until we have the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c4bca735dc733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:03:55.002384Z",
     "start_time": "2024-05-19T08:03:54.685529Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_chinese_word_freq = pd.DataFrame(word_counts_chinese.reset_index())\n",
    "df_chinese_word_freq.rename(columns={\"count\": \"frequency\"}, inplace=True)\n",
    "# normalize word frequency\n",
    "df_chinese_word_freq['frequency'] = df_chinese_word_freq['frequency'] / df_chinese_word_freq['frequency'].sum()\n",
    "\n",
    "df_english_word_freq = pd.DataFrame(word_counts_english.reset_index())\n",
    "df_english_word_freq.rename(columns={\"count\": \"frequency\"}, inplace=True)\n",
    "df_english_word_freq['frequency'] = df_english_word_freq['frequency'] / df_english_word_freq['frequency'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4eee0796e3023",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343033cd-a0da-4a7c-997c-b4b62121a4d5",
   "metadata": {},
   "source": [
    "### Browns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da516529c29d5acc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T07:15:41.111829Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's generate the original frequency of words: I will use the brown dataset from NLTK. Can also try to use COCA, it iw what Nitsan used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367be579968921f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:01.149151Z",
     "start_time": "2024-05-19T08:03:57.905233Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# Get the words from the Brown Corpus\n",
    "brown_words = brown.words()\n",
    "# get length of brown words\n",
    "total_words = len(brown_words)\n",
    "# Calculate word frequencies\n",
    "dic_brown_word_freq = nltk.FreqDist(brown_words)\n",
    "# normalize the word frequency\n",
    "dic_brown_word_freq = {word: freq / total_words for word, freq in dic_brown_word_freq.items()}\n",
    "# convert to df\n",
    "df_brown_word_freq = pd.DataFrame.from_dict(dic_brown_word_freq, orient='index').reset_index()\n",
    "df_brown_word_freq.rename(columns={'index':'terms', 0:'frequency'}, inplace=True)\n",
    "df_brown_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107f12f-c8f3-488f-bbd5-f6bc934082b8",
   "metadata": {},
   "source": [
    "### Rank by Full name\n",
    "In the most common full terms section, we have ranked all the terms. But we also need the rank of all the full names. \n",
    "Now we need the frequency for names: We have parsed a name into terms, so we will define a name as lower cased terms concatenated by underscore. And then we rank the names by frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92592c-da4f-49e1-aa38-48696e921543",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['terms'].apply('_'.join).str.lower()\n",
    "# sometimes pandas returns the view of a df, so we need to make the copy. \n",
    "df = df.copy()\n",
    "df['standarized_name'] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c490d-8b1b-4bce-96e0-9b36036b6ee2",
   "metadata": {},
   "source": [
    "Now we are going to take the standarize names and assign the frequency based on nationality. Notice that we have several frequency df right now: \n",
    "1. df_XXX_letter_len_freq: frequency of length (of name, count by letter)\n",
    "2. df_XXX_word_len_freq: frequency of length (of name, count by word)\n",
    "3. df_XXX_word_freq: frequency of word (seperate all the words and see each word's frequency)\n",
    "4. df_XXX_std_name_freq: frequency of full name (lower case and connected by underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e5292-400b-4a46-b53b-0a4c3865d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_chinese_std_name_freq = df[df['authorLocation'] == 'China']['standarized_name'].value_counts(normalize=True).sort_index()\n",
    "df_chinese_std_name_freq = pd.DataFrame(series_chinese_std_name_freq.reset_index())\n",
    "df_chinese_std_name_freq.rename(columns={\"proportion\": \"frequency\", 'standarized_name': 'terms'}, inplace=True)\n",
    "\n",
    "series_english_std_name_freq = df[df['authorLocation'] == 'USA']['standarized_name'].value_counts(normalize=True).sort_index()\n",
    "df_english_std_name_freq = pd.DataFrame(series_english_std_name_freq.reset_index())\n",
    "df_english_std_name_freq.rename(columns={\"proportion\": \"frequency\", 'standarized_name': 'terms'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6d7a98fae9290",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T07:15:41.112772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Zipf law plot 1\n",
    "Let's see the bron's frequency plot: seems like it accords to Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870a6371567a3d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:04.216857Z",
     "start_time": "2024-05-19T08:04:03.571787Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def print_zipfs_law(df_word_freq, label):\n",
    "    # assume the two columns are named \"terms\" and \"frequency\"\n",
    "    df_word_freq['rank'] = df_word_freq['frequency'].rank(ascending=False)\n",
    "    # for the same rank there might be more than one word\n",
    "    df_zipf = df_word_freq.groupby('rank').agg({'frequency': 'mean'}).reset_index()\n",
    "\n",
    "    plt.plot(df_zipf['rank'], df_zipf['frequency'], label=label)\n",
    "    \n",
    "    plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "    plt.xscale('log')  # Set y-axis to logarithmic scale\n",
    "    plt.xlabel('Rank of Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Word Frequency Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "print_zipfs_law(df_brown_word_freq, label='Brown')\n",
    "print_zipfs_law(df_chinese_word_freq, label='China - Word')\n",
    "print_zipfs_law(df_english_word_freq, label='USA - Word')\n",
    "print_zipfs_law(df_chinese_std_name_freq, label='China - Name')\n",
    "print_zipfs_law(df_english_std_name_freq, label='USA - Name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd573e82cf3cf87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "todo: Try to the log of 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25924a0c9c305993",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Rank bags and length by letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81109006f2f7515",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We know that the length of a word is related to the rank of the word, typically, the longer a word is, the less likely it will occur. Let's check if it is also true for programming languages for each language groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c3a294127a0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:08.851799Z",
     "start_time": "2024-05-19T08:04:08.657113Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_rank_bag_to_letter_len(df_word_freq):\n",
    "    # assume it have columns \"terms\" (abuse notation), and \"frequency\"\n",
    "    df_word_freq['rank'] = df_word_freq['frequency'].rank(ascending=False)\n",
    "    df_word_freq['lengthByLetter'] = df_word_freq['terms'].apply(len)\n",
    "    # map rank to bins\n",
    "    bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "    labels = ['1-10', '10-100', '100-1k', '1k-10k', '10k+']\n",
    "    df_word_freq['rankBag'] = pd.cut(df_word_freq['rank'], bins=bins, labels=labels, right=False)\n",
    "    # plot the boxplot for each bag: we don't show fliers\n",
    "    df_word_freq.boxplot(column='lengthByLetter', by='rankBag', grid=False, patch_artist=True, showfliers=False)\n",
    "    # Adding titles and labels\n",
    "    plt.title('Box Plot of Name Length by Rank Bags')\n",
    "    plt.suptitle('')\n",
    "    plt.xlabel('Rank Bags')\n",
    "    plt.ylabel('Name Length')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "plot_rank_bag_to_letter_len(df_brown_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efe7282e657f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:10.296501Z",
     "start_time": "2024-05-19T08:04:09.980307Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_chinese_word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc97c98afd0919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:12.744909Z",
     "start_time": "2024-05-19T08:04:11.574965Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_english_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939d582-1ac7-48ec-a306-550395225ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_chinese_std_name_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b960a47-22d6-4617-85fc-e54b48d4e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_english_std_name_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9de744716f619",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's see the percentage: In Nitsan's paper, it is the rank of \"whole name\" vs. the number of words in a name. I don't understand this approach, so let's ask Dror this weekend. \n",
    "For now let's do percentage of length by letter according to rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2cfe766f901cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:15.467595Z",
     "start_time": "2024-05-19T08:04:14.937735Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_rank_bag_to_letter_len(df_word_freq):\n",
    "    # assume it have columns \"terms\", and \"frequency\"\n",
    "    df_word_freq['rank'] = df_word_freq['frequency'].rank(ascending=False)\n",
    "    df_word_freq['lengthByLetter'] = df_word_freq['terms'].apply(len)\n",
    "    # map rank to bins\n",
    "    bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "    labels = ['1-10', '10-100', '100-1k', '1k-10k', '10k+']\n",
    "    df_word_freq['rankBag'] = pd.cut(df_word_freq['rank'], bins=bins, labels=labels, right=False)\n",
    "    # calculate for each rankBag, what is the number of each length number\n",
    "    pivot_df = df_word_freq.pivot_table(index='rankBag', columns='lengthByLetter', aggfunc='size', fill_value=0)\n",
    "    pivot_df = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
    "    # plot the boxplot for each bag: we don't show fliers\n",
    "    pivot_df.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "    # Adding titles and labels\n",
    "    plt.title('Percentage of Name Length (by letter) by Rank Bags')\n",
    "    plt.xlabel('Rank Bags')\n",
    "    plt.ylabel('Name Length')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Length', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_rank_bag_to_letter_len(df_brown_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae957beadd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:18.087260Z",
     "start_time": "2024-05-19T08:04:17.405399Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_chinese_word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e03a3d38f6446d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:20.987851Z",
     "start_time": "2024-05-19T08:04:19.461629Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_english_word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131a368-473b-4c44-b3c6-150fceeecc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_chinese_std_name_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962e431-b54d-467e-a374-0b7bc4355de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rank_bag_to_letter_len(df_english_std_name_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84b389-636b-4687-a3de-2449ea542068",
   "metadata": {},
   "source": [
    "### Rank bags and length by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758db3f-e64b-4770-8de0-6e3d55acf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rank_bag_to_word_len(df_word_freq):\n",
    "    # assume it have columns \"terms\" (abuse notation), and \"frequency\"\n",
    "    df_word_freq['rank'] = df_word_freq['frequency'].rank(ascending=False)\n",
    "    # we assume that we will only pass standarize name - words separated by one underscore. \n",
    "    df_word_freq['lengthByWord'] = df_word_freq['terms'].apply(lambda s: s.count('_') + 1)\n",
    "    # map rank to bins\n",
    "    bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "    labels = ['1-10', '10-100', '100-1k', '1k-10k', '10k+']\n",
    "    df_word_freq['rankBag'] = pd.cut(df_word_freq['rank'], bins=bins, labels=labels, right=False)\n",
    "    # calculate for each rankBag, what is the number of each length number\n",
    "    pivot_df = df_word_freq.pivot_table(index='rankBag', columns='lengthByWord', aggfunc='size', fill_value=0)\n",
    "    pivot_df = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
    "    # plot the boxplot for each bag: we don't show fliers\n",
    "    pivot_df.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    # Adding titles and labels\n",
    "    plt.title('Percentage of Name Length (by letter) by Rank Bags')\n",
    "    plt.xlabel('Rank Bags')\n",
    "    plt.ylabel('Name Length')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Length', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014ae3e-2ea8-4530-81af-fd3384319459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_bag_to_word_len(df_chinese_std_name_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba9f19-d1d6-4852-a04f-4f4c75e4fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_bag_to_word_len(df_english_std_name_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085d9c5d2eeef28",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96caf5e30a43dfbc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T07:15:41.120151Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Number of Real Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00943320b38cbc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's check if the words are real words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8a6de158da2b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:25.357765Z",
     "start_time": "2024-05-19T08:04:25.287700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "english_dictionary = set(words.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d59faf82aba0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:26.259656Z",
     "start_time": "2024-05-19T08:04:26.245464Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def lookup_terms(term):\n",
    "    return term.lower() in english_dictionary\n",
    "\n",
    "def percentage_of_real_word(df_word_frequency):\n",
    "    # assuming that the df_word_frequency have \"terms\" column\n",
    "    df_word_frequency['real_word'] = df_word_frequency['terms'].apply(lookup_terms)\n",
    "    return df_word_frequency['real_word'].mean() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5cb46cacb3c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:28.556782Z",
     "start_time": "2024-05-19T08:04:27.677506Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Real Word Percentage in Chinese Projects:\")\n",
    "print(percentage_of_real_word(df_chinese_word_freq))\n",
    "print(\"Real Word Percentage in English Projects:\")\n",
    "print(percentage_of_real_word(df_english_word_freq))\n",
    "print(\"Real Word Percentage in Brown:\")\n",
    "print(percentage_of_real_word(df_brown_word_freq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b8fee6568b04c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T07:15:41.123008Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Size of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f9e6921cabd39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T08:04:34.765226Z",
     "start_time": "2024-05-19T08:04:33.618589Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"The size of vocabulary of Chinese-speaking projects is\")\n",
    "print(flattened_words_chinese.nunique())\n",
    "print(\"The size of vocabulary of English-speaking projects is\")\n",
    "print(flattened_words_english.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0121dbb7dff5f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T06:44:27.664416Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
